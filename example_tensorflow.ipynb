{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook Example on ARC using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first optional step is to disable TensorFlow logging output. This can make the output cells of the Jupyter notebook very cluttered. However, it can be helpful to see the verbose logging output to determine if CUDA libraries are loaded. If you're using TensorFlow on your system for the first time, I recommend __keeping__ the logging so you can understand what TensorFlow is doing. If you are familar with how it works, then you can disable logging using the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/a/64438413"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we need to first import TensorFlow. Here, we alias it as `tf` to make typing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow makes working with GPUs very easy. As long as a GPU is visible to TensorFlow it can use it. Here we retreive the list of GPUs that are visible to TensorFlow as a `list` of ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n",
      "Name: /physical_device:GPU:0, Type: GPU\n"
     ]
    }
   ],
   "source": [
    "# Get list of GPU objects.\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Print the number of GPUs that are visible to TensorFlow.\n",
    "print(f\"Num GPUs Available: {len(gpus)}\")\n",
    "\n",
    "# Loop over all GPUs and print their name and device type.\n",
    "for gpu in gpus:\n",
    "    print(f\"Name: {gpu.name}, Type: {gpu.device_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some basic tensor operations on the GPU specifically. There are multiple ways of doing this. By default TensorFlow automatically uses the first GPU in its available list, otherwise it uses the CPU. So, simply perform tensor operations and they will automatically be redirected to the appropriate device!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log when operations are done on GPU.\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "c.shape=TensorShape([10, 3])\n"
     ]
    }
   ],
   "source": [
    "# Run operation on available hardware.\n",
    "a = tf.zeros((10,2)) # Left matrix with 10 rows and 2 columns\n",
    "b = tf.zeros((2,3)) # Right matrix with 2 rows and 3 columns\n",
    "c = tf.tensordot(a, b, axes=1) # Matrix dot product results in 10 rows and 3 columns\n",
    "print(f\"{c.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are of course other ways of running on GPUs. The next easiest way is to manually tell TensorFlow which GPU you want to use. To do this, we use a Python context manager called `tf.device()` which runs operations within its context on the device you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "c.shape=TensorShape([10, 3])\n"
     ]
    }
   ],
   "source": [
    "# Iterate over GPUs and perform same computation on each separately.\n",
    "for gpu in gpus:\n",
    "\n",
    "    # Use context manager to have computation done on GPU.\n",
    "    # with tf.device(gpu.name.replace('physical_device', 'device')):\n",
    "    with tf.device(gpu.name.split(':', 1)[1]):\n",
    "        a = tf.zeros((10,2)) # Left matrix with 10 rows and 2 columns\n",
    "        b = tf.zeros((2,3)) # Right matrix with 2 rows and 3 columns\n",
    "        c = tf.tensordot(a, b, axes=1) # Matrix dot product results in 10 rows and 3 columns\n",
    "        print(f\"{c.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of doing this is to use TensorFlow `Strategy` objects, which allow more advanced placement of where computations occurr (either on CPU, single GPU, or multiple GPUs in parallel). To start, let's use just a single GPU here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "c.shape=TensorShape([10, 3])\n"
     ]
    }
   ],
   "source": [
    "# To make things simple, let's create strategy for a single GPU\n",
    "# (the first one in our list, at device index \"0\")\n",
    "strategy = tf.distribute.OneDeviceStrategy('gpu:0')\n",
    "\n",
    "# Use context manager to have computation done on GPU.\n",
    "with strategy.scope():\n",
    "    a = tf.zeros((10,2)) # Left matrix with 10 rows and 2 columns\n",
    "    b = tf.zeros((2,3)) # Right matrix with 2 rows and 3 columns\n",
    "    c = tf.tensordot(a, b, axes=1) # Matrix dot product results in 10 rows and 3 columns\n",
    "    print(f\"{c.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say you need more compute power. If you have multuple GPUs available, you can change the strategy to suport multiple GPUs in __parallel__. There are also multiple ways of doing this, all are covered in the [TensorFlow documentation](https://www.tensorflow.org/guide/gpu#:~:text=Note%3A%20Use%20tf.,machines%2C%20is%20using%20Distribution%20Strategies). The simplest way is to use a `MirroredStrategy`. In more complex settings, this strategy replicates a an AI model on each GPU and splits the batches across multiple GPUs, thus efficiently parallelizing computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "c.shape=TensorShape([10, 3])\n"
     ]
    }
   ],
   "source": [
    "# Create a mirrored strategy using all GPUs visible to TensorFlow.\n",
    "strategy = tf.distribute.MirroredStrategy([\n",
    "    gpu.name.split(':', 1)[1] for gpu in gpus\n",
    "])\n",
    "\n",
    "# Use context manager to have computation done on GPU.\n",
    "with strategy.scope():\n",
    "    a = tf.zeros((10,2)) # Left matrix with 10 rows and 2 columns\n",
    "    b = tf.zeros((2,3)) # Right matrix with 2 rows and 3 columns\n",
    "    c = tf.tensordot(a, b, axes=1) # Matrix dot product results in 10 rows and 3 columns\n",
    "    print(f\"{c.shape=}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9d28b892dbcf26bab028d0e176e36bb3090c4acb3765664e3f159d722d1b85e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf-p100')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
